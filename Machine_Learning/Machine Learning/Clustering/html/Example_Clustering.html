
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Clustering Corporate Bonds</title><meta name="generator" content="MATLAB 8.1"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-07-18"><meta name="DC.source" content="Example_Clustering.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Clustering Corporate Bonds</h1><!--introduction--><p>Clustering is a form of unsupervised learning technique. The purpose of clustering is to identify natural groupings of data from a large data set to produce a concise representation based on common characteristics. Cluster analysis has wide ranging applications including computational biology, climatology, psychology and medicine, social network analysis, business and marketing. Clustering may be employed for various reasons including data summarization, compression, efficiently finding nearest neighbors, identifying similar objects etc.</p><p>In this example, several unsupervised machine learning techniques available in MATLAB are highlighted. One may apply one or more of the techniques to partition their data in different interesting ways.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Description of the Data</a></li><li><a href="#2">Import Data</a></li><li><a href="#3">Slice Data</a></li><li><a href="#4">Visualize Data</a></li><li><a href="#5">Select Features to use for Clustering</a></li><li><a href="#6">Speed up Computations using Parallel Computing</a></li><li><a href="#7">Clustering Techniques</a></li><li><a href="#8">Partitional Clustering</a></li><li><a href="#9">k-Means Clustering</a></li><li><a href="#10">Hierarchical Clustering</a></li><li><a href="#11">Neural Networks - Self Organizing Maps (SOMs)</a></li><li><a href="#12">Overlapping Clustering</a></li><li><a href="#13">Fuzzy C-Means Clustering</a></li><li><a href="#14">Probability Plot for Fuzzy C-means</a></li><li><a href="#15">Gaussian Mixture Models (GMM)</a></li><li><a href="#16">Probability Plot for GMM</a></li><li><a href="#17">Cluster Evaluation</a></li><li><a href="#18">Visualize Similarity Matrix</a></li><li><a href="#19">Hierarchical Clustering: Cophenetic Corr. Coeff. and Dendrogram</a></li><li><a href="#20">k-Means Clustering: Determining Correct Number of Clusters</a></li><li><a href="#21">Shut Down Workers</a></li></ul></div><h2>Description of the Data<a name="1"></a></h2><p>We have simulated some data to reflect characteristics of Corporate and other kinds of bonds.</p><p>The clustering goal is to segment the corporate bond data using distance based and probability based clustering techniques.</p><p>% Attributes:</p><div><ol><li>Type : type of bond (categorical: Corp, Muni, Treas, Zero)</li><li>Name : name of company issuing the bond (string)</li><li>Price : market price of the bond (numeric)</li><li>Coupon : coupon rate of the bond (numeric)</li><li>Maturity : maturity date of the bond (string date)</li><li>YTM : yield to maturity of the bond (numeric)</li><li>CurrentYield : current yield of the bond</li><li>Rating : credit rating of the bond (categorical: AAA, AA, A, BBB, BB, B, CCC, CC, D, NA, Not Rated)</li><li>Callable: bond is callable or not (binary: 0 or 1)</li></ol></div><h2>Import Data<a name="2"></a></h2><p>Import the data into MATLAB. Here we have the data previously pre-processed and stored into a MAT-file. However, one may import the data from a different source and pre-process it before applying any clustering techniques.</p><pre class="codeinput">load <span class="string">BondData</span>
settle = floor(date);
</pre><h2>Slice Data<a name="3"></a></h2><p>In this case, we will only work with corporate bonds with a rating of CC or higher. Amongst the corporate bonds, we only keep those which will mature after the settle date and whose YTM is greater than zero and less than 30.</p><pre class="codeinput"><span class="comment">% Add numeric maturity date and settle date columns to dataset</span>
bondData.MaturityN = datenum(bondData.Maturity, <span class="string">'dd-mmm-yyyy'</span>);
bondData.SettleN = settle * ones(length(bondData),1);

<span class="comment">% Dataset arrays make it easier to slice the data as per our needs</span>
corp = bondData(bondData.MaturityN &gt; settle &amp; <span class="keyword">...</span>
             bondData.Type == <span class="string">'Corp'</span> &amp; <span class="keyword">...</span>
             bondData.Rating &gt;= <span class="string">'CC'</span> &amp; <span class="keyword">...</span>
             bondData.YTM &lt; 30 &amp; <span class="keyword">...</span>
             bondData.YTM &gt;= 0, :);
<span class="comment">% Set the random number seed to make the results repeatable in this script</span>
rng(<span class="string">'default'</span>);
</pre><h2>Visualize Data<a name="4"></a></h2><p>One can open the variable <tt>corp</tt> or <tt>bondData</tt>, in the Variable Editor and interactively create different kinds of plots by selecting 1 or more columns.</p><p>As one creates the plots, MATLAB tries to help by echoing the commands on to the Command Window.</p><pre class="codeinput"><span class="comment">% Coupon Rate vs. YTM plot, differentiated by credit rating</span>
gscatter(corp.Coupon,corp.YTM,corp.Rating)
<span class="comment">% Label the plot</span>
xlabel(<span class="string">'Coupon Rate'</span>)
ylabel(<span class="string">'YTM'</span>)
<span class="comment">% Here we can see that bonds with higher ratings have lower YTM and coupon</span>
<span class="comment">% rates and vice versa, as one may expect.</span>
</pre><img vspace="5" hspace="5" src="Example_Clustering_01.png" alt=""> <h2>Select Features to use for Clustering<a name="5"></a></h2><p>In this case, we expect that Coupon Rate, Yield-to-Maturity, Current Yield and Credit Rating should be sufficient to cluster the data.</p><p>Additionally, for now, we will use the clustering techniques to partition the data into 3 clusters. We will look at ways to help us decide the appropriate number of clusters, later.</p><pre class="codeinput"><span class="comment">% Features</span>
bonds = double(corp(:,[4,6:8]));
<span class="comment">% Number of Clusters to create</span>
numClust = 3;
</pre><h2>Speed up Computations using Parallel Computing<a name="6"></a></h2><p>If Parallel Computing Toolbox is available, the computation will be distributed to 2 workers for speeding up the evaluation.</p><pre class="codeinput"><span class="keyword">if</span> matlabpool(<span class="string">'size'</span>) == 0
    matlabpool <span class="string">open</span> <span class="string">2</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">Starting matlabpool using the 'local' profile ... connected to 2 workers.
</pre><h2>Clustering Techniques<a name="7"></a></h2><p>The definition of a <tt>cluster</tt> is imprecise and the best definition depends on the nature of the data and the desired results.</p><p>The common clustering techniques discussed in literature include distance-based, probability-based and density-based methods. In this example, we will discuss distance-based and probability-based techniques.</p><h2>Partitional Clustering<a name="8"></a></h2><p>In case of <b>partitional clustering</b> , the data points are divided into non-overlapping sets (clusters) such that each data point is part of exactly one set, for instance, k-Means clustering or self-organizing maps (SOMs). If each cluster may have subclusters, then it is called hierarchical clustering.</p><h2>k-Means Clustering<a name="9"></a></h2><p>k-Means clustering is a partitioning method. The function <i>kmeans</i> partitions data into k mutually exclusive clusters, and returns the index of the cluster to which it has assigned each observation.</p><p><i>kmeans</i> uses an iterative algorithm that minimizes the sum of distances from each object to it's cluster centroid, over all clusters. A variety of distance measures such as correlation, hamming, cityBlock etc. are available.</p><pre class="codeinput">dist_k = <span class="string">'cosine'</span>;
options_km = statset(<span class="string">'UseParallel'</span>, true);
kidx = kmeans(bonds, numClust, <span class="string">'distance'</span>, dist_k, <span class="string">'options'</span>, options_km);

<span class="comment">% Visualize results</span>
plotBondClusters(corp, kidx, <span class="string">'k-Means Clustering'</span>)
</pre><img vspace="5" hspace="5" src="Example_Clustering_02.png" alt=""> <h2>Hierarchical Clustering<a name="10"></a></h2><p>Hierarchical Clustering groups data over a variety of scales by creating a cluster tree or dendrogram. The tree is not a single set of clusters, but rather a multilevel hierarchy, where clusters at one level are joined as clusters at the next level. A variety of distance measures such as Euclidean, Mahalanobis, Jaccard etc. are available.</p><pre class="codeinput">dist_h = <span class="string">'spearman'</span>;
link = <span class="string">'weighted'</span>;
hidx = clusterdata(bonds, <span class="string">'maxclust'</span>, numClust, <span class="string">'distance'</span> , dist_h, <span class="string">'linkage'</span>, link);

<span class="comment">% Visualize results</span>
plotBondClusters(corp, hidx, <span class="string">'Hierarchical Clustering'</span>)
</pre><img vspace="5" hspace="5" src="Example_Clustering_03.png" alt=""> <h2>Neural Networks - Self Organizing Maps (SOMs)<a name="11"></a></h2><p>Neural Network Toolbox supports unsupervised learning with self-organizing maps (SOMs) and competitive layers.</p><p>SOMs learn to classify input vectors according to how they are grouped in the input space. SOMs learn both the distribution and topology of the input vectors they are trained on. One can make use of the interactive tools to setup, train and test a neural network. It is then possible to auto-generate the code for the purpose of automation. The code in this section has been auto-generated.</p><pre class="codeinput"><span class="comment">% Create a Self-Organizing Map</span>
dimension1 = 3;
dimension2 = 1;
net = selforgmap([dimension1 dimension2]);

<span class="comment">% Train the Network</span>
net.trainParam.showWindow = 0;
[net,tr] = train(net,bonds');

<span class="comment">% Test the Network</span>
nidx = net(bonds');
nidx = vec2ind(nidx)';

<span class="comment">% Visualize results</span>
plotBondClusters(corp, nidx, <span class="string">'SOM'</span>)
</pre><img vspace="5" hspace="5" src="Example_Clustering_04.png" alt=""> <h2>Overlapping Clustering<a name="12"></a></h2><p>While clustering, if a data point can simultaneously exist in more than one clusters, it is known as <b>overlapping or fuzzy clustering</b>, for example, fuzzy C-means clustering or Gaussian mixture models. A probability or membership weight (ranging from 0 to 1) is assigned to each data point. Often, an additional constraint, that is, the sum of weights is equal to 1, is imposed.</p><h2>Fuzzy C-Means Clustering<a name="13"></a></h2><p>Fuzzy C-means (FCM) is a data clustering technique wherein each data point belongs to a cluster to some degree that is specified by a membership grade. The function <i>fcm</i> returns a list of cluster centers and several membership grades for each data point.</p><p><i>fcm</i> iteratively minimizes the sum of distances of each data point to its cluster center weighted by that data point's membership grade.</p><pre class="codeinput">options = nan(4,1);
options(4) = 0;
<span class="comment">% Hide iteration information by passing appropriate options to FCM</span>
[centres,U] = fcm(bonds,numClust, options);
[~, fidx] = max(U);
fidx = fidx';

<span class="comment">% Visualize results</span>
plotBondClusters(corp, fidx, <span class="string">'Fuzzy C-Means Clustering'</span>)
</pre><img vspace="5" hspace="5" src="Example_Clustering_05.png" alt=""> <h2>Probability Plot for Fuzzy C-means<a name="14"></a></h2><p>Here, we visualize the membership grades between different clusters along two dimensions, viz. credit ratings and coupon rates. One may create similar plots along other dimensions as well.</p><pre class="codeinput">plotClusterProb(numClust, corp, U', <span class="string">'FCM'</span> )
</pre><img vspace="5" hspace="5" src="Example_Clustering_06.png" alt=""> <h2>Gaussian Mixture Models (GMM)<a name="15"></a></h2><p>Gaussian mixture models are formed by combining multivariate normal density components. <i>gmdistribution.fit</i> uses expectation maximization (EM) algorithm, which assigns posterior probabilities to each component density with respect to each observation. The posterior probabilities for each point indicate that each data point has some probability of belonging to each cluster.</p><pre class="codeinput">gmobj = gmdistribution.fit(bonds,numClust);
gidx = cluster(gmobj,bonds);

<span class="comment">% Visualize results</span>
plotBondClusters(corp, gidx, <span class="string">'Gaussian Mixture Mode'</span>)
</pre><img vspace="5" hspace="5" src="Example_Clustering_07.png" alt=""> <h2>Probability Plot for GMM<a name="16"></a></h2><p>Here, we visualize the probability that a data point belongs to a particular cluster, along two dimensions, viz. credit ratings and coupon rates. One may create similar plots along other dimensions as well.</p><pre class="codeinput">P = posterior(gmobj,bonds);
plotClusterProb(numClust, corp, P, <span class="string">'GMM'</span> , <span class="string">'2D'</span>)
</pre><img vspace="5" hspace="5" src="Example_Clustering_08.png" alt=""> <h2>Cluster Evaluation<a name="17"></a></h2><p>Almost every clustering algorithm will find clusters in a data set, even if that data set has no natural cluster structure. It is important to be able to distinguish whether there is non-random structure in the data. Also, how does one determine the correct number of clusters existing in the data?</p><h2>Visualize Similarity Matrix<a name="18"></a></h2><p>One may be able to judge clustering visually by plotting its similarity matrix. If we have well separated clusters, then if we re-order the similarity matrix based on cluster labels and plot it, we would expect it should be roughly block diagonal.</p><pre class="codeinput">[dist_metric_h, dist_metric_k] = plotSimilarityMatrix(bonds, dist_h, hidx, dist_k, kidx );
</pre><img vspace="5" hspace="5" src="Example_Clustering_09.png" alt=""> <h2>Hierarchical Clustering: Cophenetic Corr. Coeff. and Dendrogram<a name="19"></a></h2><p>Cophenetic correlation coefficient is typically used to evaluate which type of hierarchical clustering is more suitable for a particular type of data. It is a measure of how faithfully the tree represents the similarities/dissimilarities among observations.</p><p>Dendrogram is a graphical representation of the cluster tree created by hierarchical clustering.</p><pre class="codeinput">Z = linkage(dist_metric_h,link);
cpcc = cophenet(Z,dist_metric_h);
disp(<span class="string">'Cophenetic correlation coefficient: '</span>)
disp(cpcc)

set(0,<span class="string">'RecursionLimit'</span>,5000)
figure
dendrogram(Z)
set(0,<span class="string">'RecursionLimit'</span>,500)
xlabel(<span class="string">'Data point index'</span>)
ylabel (<span class="string">'Distance b/w data points'</span>)
title([<span class="string">'CPCC: '</span> sprintf(<span class="string">'%0.4f'</span>,cpcc)])
</pre><pre class="codeoutput">Cophenetic correlation coefficient: 
      0.89029
</pre><img vspace="5" hspace="5" src="Example_Clustering_10.png" alt=""> <h2>k-Means Clustering: Determining Correct Number of Clusters<a name="20"></a></h2><p>To get an idea of how well-separated the clusters are, one can make use of silhouette plot. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters. This measure ranges from +1, indicating points that are very distant from neighboring clusters, through 0, indicating points that are not distinctly in one cluster or another, to -1, indicating points that are probably assigned to the wrong cluster.</p><p>A more quantitative way to compare the silhouette values for different number of clusters is to look at the average silhouette values in each case.</p><pre class="codeinput"><span class="comment">% Plot the silhouette values for k-means clustering by creating 2 and 3</span>
<span class="comment">% clusters respectively</span>
<span class="keyword">for</span> i=2:3
    figure
    kidx = kmeans(bonds,i,<span class="string">'distance'</span>,dist_k,<span class="string">'options'</span>,options_km);
    [~,h] = silhouette(bonds,kidx,dist_k);
    title([<span class="string">'Silhouette values with '</span> num2str(i) <span class="string">' clusters'</span>])
    snapnow
<span class="keyword">end</span>

<span class="comment">% Compute and plot the average silhouette values for 2 through 10 clusters</span>
numC = 10;
silh_m = nan(numC,1);

<span class="keyword">for</span> i=1:numC
    kidx = kmeans(bonds,i,<span class="string">'distance'</span>,dist_k,<span class="string">'options'</span>,options_km);
    silh = silhouette(bonds,kidx,dist_k);
    silh_m(i) = mean(silh);
<span class="keyword">end</span>

figure
plot(1:numC,silh_m,<span class="string">'o-'</span>)
xlabel(<span class="string">'Number of Clusters'</span>)
ylabel(<span class="string">'Mean Silhouette Value'</span>)
title(<span class="string">'Average Silhouette Values vs. Number of Clusters'</span>)
</pre><img vspace="5" hspace="5" src="Example_Clustering_11.png" alt=""> <img vspace="5" hspace="5" src="Example_Clustering_12.png" alt=""> <pre class="codeoutput">Warning: Failed to converge in 100 iterations.
&gt; In kmeans&gt;loopBody at 391
  In smartForReduce&gt;(parfor body) at 111
  In parallel_function&gt;make_general_channel/channel_general at 900
  In remoteParallelFunction at 28 
</pre><img vspace="5" hspace="5" src="Example_Clustering_13.png" alt=""> <h2>Shut Down Workers<a name="21"></a></h2><p>Release the workers if there is no more work for them</p><pre class="codeinput"><span class="keyword">if</span> matlabpool(<span class="string">'size'</span>) &gt; 0
    matlabpool <span class="string">close</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">Sending a stop signal to all the workers ... stopped.
</pre><p class="footer">Copyright 2013 The MathWorks, Inc.<br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2013a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Clustering Corporate Bonds
% Clustering is a form of unsupervised learning technique. The purpose of
% clustering is to identify natural groupings of data from a large data set
% to produce a concise representation based on common characteristics.
% Cluster analysis has wide ranging applications including computational
% biology, climatology, psychology and medicine, social network analysis,
% business and marketing. Clustering may be employed for various reasons
% including data summarization, compression, efficiently finding nearest
% neighbors, identifying similar objects etc.
% 
% In this example, several unsupervised machine learning techniques
% available in MATLAB are highlighted. One may apply one or more of the
% techniques to partition their data in different interesting ways.

% Copyright 2013 The MathWorks, Inc.

%% Description of the Data
% We have simulated some data to reflect characteristics of Corporate and
% other kinds of bonds. 
% 
% The clustering goal is to segment the corporate bond data using distance
% based and probability based clustering techniques.
% 
% % Attributes:
% 
% # Type : type of bond (categorical: Corp, Muni, Treas, Zero)
% # Name : name of company issuing the bond (string)
% # Price : market price of the bond (numeric)
% # Coupon : coupon rate of the bond (numeric)
% # Maturity : maturity date of the bond (string date)
% # YTM : yield to maturity of the bond (numeric)
% # CurrentYield : current yield of the bond
% # Rating : credit rating of the bond (categorical: AAA, AA, A, BBB, BB, B, CCC, CC, D, NA, Not Rated)
% # Callable: bond is callable or not (binary: 0 or 1)

%% Import Data
% Import the data into MATLAB. Here we have the data previously
% pre-processed and stored into a MAT-file. However, one may import the
% data from a different source and pre-process it before applying any
% clustering techniques.

load BondData
settle = floor(date);

%% Slice Data
% In this case, we will only work with corporate bonds with a rating of CC
% or higher. Amongst the corporate bonds, we only keep those which will
% mature after the settle date and whose YTM is greater than zero and less
% than 30.

% Add numeric maturity date and settle date columns to dataset
bondData.MaturityN = datenum(bondData.Maturity, 'dd-mmm-yyyy');
bondData.SettleN = settle * ones(length(bondData),1);

% Dataset arrays make it easier to slice the data as per our needs
corp = bondData(bondData.MaturityN > settle & ...
             bondData.Type == 'Corp' & ...
             bondData.Rating >= 'CC' & ...
             bondData.YTM < 30 & ...
             bondData.YTM >= 0, :);
% Set the random number seed to make the results repeatable in this script
rng('default');

%% Visualize Data
% One can open the variable |corp| or |bondData|, in the Variable Editor
% and interactively create different kinds of plots by selecting 1 or more
% columns.
% 
% As one creates the plots, MATLAB tries to help by echoing the commands on
% to the Command Window.

% Coupon Rate vs. YTM plot, differentiated by credit rating
gscatter(corp.Coupon,corp.YTM,corp.Rating)
% Label the plot
xlabel('Coupon Rate')
ylabel('YTM')
% Here we can see that bonds with higher ratings have lower YTM and coupon
% rates and vice versa, as one may expect.

%% Select Features to use for Clustering
% In this case, we expect that Coupon Rate, Yield-to-Maturity, Current
% Yield and Credit Rating should be sufficient to cluster the data. 
% 
% Additionally, for now, we will use the clustering techniques to partition
% the data into 3 clusters. We will look at ways to help us decide the
% appropriate number of clusters, later.

% Features
bonds = double(corp(:,[4,6:8]));
% Number of Clusters to create
numClust = 3;

%% Speed up Computations using Parallel Computing
% If Parallel Computing Toolbox is available, the computation will be
% distributed to 2 workers for speeding up the evaluation.

if matlabpool('size') == 0 
    matlabpool open 2
end

%% Clustering Techniques
% The definition of a |cluster| is imprecise and the best definition
% depends on the nature of the data and the desired results.
% 
% The common clustering techniques discussed in literature include
% distance-based, probability-based and density-based methods. In this
% example, we will discuss distance-based and probability-based techniques.

%% Partitional Clustering
% In case of *partitional clustering* , the data points are divided into
% non-overlapping sets (clusters) such that each data point is part of
% exactly one set, for instance, k-Means clustering or self-organizing
% maps (SOMs). If each cluster may have subclusters, then it is called
% hierarchical clustering.

%% k-Means Clustering
% k-Means clustering is a partitioning method. The function _kmeans_
% partitions data into k mutually exclusive clusters, and returns the index
% of the cluster to which it has assigned each observation.
% 
% _kmeans_ uses an iterative algorithm that minimizes the sum of distances
% from each object to it's cluster centroid, over all clusters. A variety of
% distance measures such as correlation, hamming, cityBlock etc. are
% available.

dist_k = 'cosine';
options_km = statset('UseParallel', true);
kidx = kmeans(bonds, numClust, 'distance', dist_k, 'options', options_km);

% Visualize results
plotBondClusters(corp, kidx, 'k-Means Clustering')

%% Hierarchical Clustering
% Hierarchical Clustering groups data over a variety of scales by creating
% a cluster tree or dendrogram. The tree is not a single set of clusters,
% but rather a multilevel hierarchy, where clusters at one level are joined
% as clusters at the next level. A variety of distance measures such as
% Euclidean, Mahalanobis, Jaccard etc. are available.

dist_h = 'spearman';
link = 'weighted';
hidx = clusterdata(bonds, 'maxclust', numClust, 'distance' , dist_h, 'linkage', link);

% Visualize results
plotBondClusters(corp, hidx, 'Hierarchical Clustering')

%% Neural Networks - Self Organizing Maps (SOMs)
% Neural Network Toolbox supports unsupervised learning with
% self-organizing maps (SOMs) and competitive layers.
% 
% SOMs learn to classify input vectors according to how they are grouped in
% the input space. SOMs learn both the distribution and topology of the
% input vectors they are trained on. One can make use of the interactive
% tools to setup, train and test a neural network. It is then possible to
% auto-generate the code for the purpose of automation. The code in this
% section has been auto-generated. 

% Create a Self-Organizing Map
dimension1 = 3;
dimension2 = 1;
net = selforgmap([dimension1 dimension2]);

% Train the Network
net.trainParam.showWindow = 0;
[net,tr] = train(net,bonds');

% Test the Network
nidx = net(bonds');
nidx = vec2ind(nidx)';

% Visualize results
plotBondClusters(corp, nidx, 'SOM')

%% Overlapping Clustering
% While clustering, if a data point can simultaneously exist in more than
% one clusters, it is known as *overlapping or fuzzy clustering*, for
% example, fuzzy C-means clustering or Gaussian mixture models. A
% probability or membership weight (ranging from 0 to 1) is assigned to
% each data point. Often, an additional constraint, that is, the sum of
% weights is equal to 1, is imposed.

%% Fuzzy C-Means Clustering
% Fuzzy C-means (FCM) is a data clustering technique wherein each data
% point belongs to a cluster to some degree that is specified by a
% membership grade. The function _fcm_ returns a list of cluster centers
% and several membership grades for each data point.
% 
% _fcm_ iteratively minimizes the sum of distances of each data point to
% its cluster center weighted by that data point's membership grade.

options = nan(4,1);
options(4) = 0;
% Hide iteration information by passing appropriate options to FCM
[centres,U] = fcm(bonds,numClust, options);
[~, fidx] = max(U);
fidx = fidx';

% Visualize results
plotBondClusters(corp, fidx, 'Fuzzy C-Means Clustering')

%% Probability Plot for Fuzzy C-means
% Here, we visualize the membership grades between different clusters along
% two dimensions, viz. credit ratings and coupon rates. One may create
% similar plots along other dimensions as well.

plotClusterProb(numClust, corp, U', 'FCM' )

%% Gaussian Mixture Models (GMM)
% Gaussian mixture models are formed by combining multivariate normal
% density components. _gmdistribution.fit_ uses expectation maximization
% (EM) algorithm, which assigns posterior probabilities to each component
% density with respect to each observation. The posterior probabilities for
% each point indicate that each data point has some probability of
% belonging to each cluster.

gmobj = gmdistribution.fit(bonds,numClust);
gidx = cluster(gmobj,bonds);

% Visualize results
plotBondClusters(corp, gidx, 'Gaussian Mixture Mode')

%% Probability Plot for GMM
% Here, we visualize the probability that a data point belongs to a
% particular cluster, along two dimensions, viz. credit ratings and coupon
% rates. One may create similar plots along other dimensions as well.

P = posterior(gmobj,bonds);
plotClusterProb(numClust, corp, P, 'GMM' , '2D')

%% Cluster Evaluation
% Almost every clustering algorithm will find clusters in a data set, even
% if that data set has no natural cluster structure. It is important to be
% able to distinguish whether there is non-random structure in the data.
% Also, how does one determine the correct number of clusters existing in
% the data? 

%% Visualize Similarity Matrix
% One may be able to judge clustering visually by plotting its similarity
% matrix. If we have well separated clusters, then if we re-order the
% similarity matrix based on cluster labels and plot it, we would expect it
% should be roughly block diagonal.

[dist_metric_h, dist_metric_k] = plotSimilarityMatrix(bonds, dist_h, hidx, dist_k, kidx );

%% Hierarchical Clustering: Cophenetic Corr. Coeff. and Dendrogram
% Cophenetic correlation coefficient is typically used to evaluate which
% type of hierarchical clustering is more suitable for a particular type of
% data. It is a measure of how faithfully the tree represents the
% similarities/dissimilarities among observations.
% 
% Dendrogram is a graphical representation of the cluster tree created by
% hierarchical clustering.

Z = linkage(dist_metric_h,link);
cpcc = cophenet(Z,dist_metric_h);
disp('Cophenetic correlation coefficient: ')
disp(cpcc)

set(0,'RecursionLimit',5000)
figure
dendrogram(Z)
set(0,'RecursionLimit',500)
xlabel('Data point index')
ylabel ('Distance b/w data points')
title(['CPCC: ' sprintf('%0.4f',cpcc)])

%% k-Means Clustering: Determining Correct Number of Clusters
% To get an idea of how well-separated the clusters are, one can make use
% of silhouette plot. The silhouette plot displays a measure of how close
% each point in one cluster is to points in the neighboring clusters. This
% measure ranges from +1, indicating points that are very distant from
% neighboring clusters, through 0, indicating points that are not
% distinctly in one cluster or another, to -1, indicating points that are
% probably assigned to the wrong cluster.
% 
% A more quantitative way to compare the silhouette values for different
% number of clusters is to look at the average silhouette values in each
% case.

% Plot the silhouette values for k-means clustering by creating 2 and 3
% clusters respectively
for i=2:3
    figure
    kidx = kmeans(bonds,i,'distance',dist_k,'options',options_km);
    [~,h] = silhouette(bonds,kidx,dist_k);
    title(['Silhouette values with ' num2str(i) ' clusters'])
    snapnow
end

% Compute and plot the average silhouette values for 2 through 10 clusters
numC = 10;
silh_m = nan(numC,1);

for i=1:numC
    kidx = kmeans(bonds,i,'distance',dist_k,'options',options_km);
    silh = silhouette(bonds,kidx,dist_k);
    silh_m(i) = mean(silh);
end

figure
plot(1:numC,silh_m,'o-')
xlabel('Number of Clusters')
ylabel('Mean Silhouette Value')
title('Average Silhouette Values vs. Number of Clusters')

%% Shut Down Workers
% Release the workers if there is no more work for them

if matlabpool('size') > 0
    matlabpool close
end
##### SOURCE END #####
--></body></html>